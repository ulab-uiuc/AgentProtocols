# GAIA general configuration (converted from config.example.toml)
# Notes:
# - The llm.* sections below correspond to [llm] / [llm.vision] in the original TOML
# - Optional blocks (browser / search / sandbox / runflow, etc.) are kept commented out; enable as needed
# - If integrating with an AppConfig/Config loader, you may adjust key names accordingly
planner:
  reuse_plan: true  # Whether to reuse an existing plan

llm:
  default:
    model: "gpt-4o"          # Primary model
    base_url: "https://api.openai.com/v1"     # API base URL
    api_key: ""              # Set via OPENAI_API_KEY environment variable
    max_tokens: 8192         # Maximum output tokens
    temperature: 0.0         # Temperature (controls randomness)
    # api_type: ""           # Optional: aws / azure / ollama, etc.
    # api_version: ""        # Azure OpenAI version (only for api_type=azure)

  vision:
    model: "gpt-4o"          # Vision model (remove this block if not needed)
    base_url: "https://api.openai.com/v1"
    api_key: ""              # Set via OPENAI_API_KEY environment variable
    max_tokens: 8192
    temperature: 0.0

# ================== MCP settings (mcp) ==================
# mcp:
#   server_reference: "app.mcp.server"   # Module entry (enabled in example)

mcp:
  server_reference: "app.mcp.server"

# ================== Runflow workflow extensions (runflow) ==================
runflow:
  use_data_analysis_agent: false

# ================== Backwards compatibility: original simple model block (remove if not needed) ==================
protocol: "a2a"
mode: "normal" # debug or normal 

# Runtime configuration
runtime:
  task_file: "scenarios/gaia/dataset/2023/validation/metadata.jsonl"
  output_file: "workspaces/gaia_a2a_results.json"
  timeout: 300  # seconds per task

# Framework configuration
framework:
  name: "gpt-4o"
  version: "1.0.0"

network:
  host: "127.0.0.1"
  port_range:
    start: 9000
    end: 9100
  timeout_seconds: 60
  max_connections: 10

agents:
  default_max_tokens: 5000
  default_priority: 1
  workspace_base: "workspaces"
  max_agent_num: 10
  agent_timeout: 10000  # milliseconds
  
  # GAIA level-based agent recommendations
  level_recommendations:
    1: 2  # Level 1: Simple tasks, 2 agents recommended
    2: 4  # Level 2: Medium tasks, 4 agents recommended  
    3: 8  # Level 3: Complex tasks, 8 agents recommended
  
  complexity_thresholds:
    low: 2000
    medium: 4000
    high: 8000

performance:
  max_execution_time: 300000  # milliseconds
  target_accuracy: 0.8
  max_total_tokens: 32768

evaluation:
  truth_file: "ground_truth.json"
  metrics_file: "metrics.json"
  artifacts_file: "run_artifacts.tar.gz"
  judge_timeout: 30  # LLM judge evaluation timeout in seconds

logging:
  level: "INFO"
  format: "[%(asctime)s] %(levelname)s: %(message)s"
  agent_logs: true
  packet_logs: true

model:
  api_type: "openai"       # API type: openai, anthropic, or google
  name: "gpt-4o"
  base_url: "https://api.openai.com/v1"
  api_key: ""              # Set via OPENAI_API_KEY environment variable
  max_tokens: 4096
  temperature: 0.0
  timeout: 30
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0

# ================== Optional multi-vendor (examples - uncomment as needed) ==================
# llm:
#   aws:
#     api_type: "aws"
#     model: "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
#     base_url: "bedrock-runtime.us-west-2.amazonaws.com"  # Currently unused; can be left blank
#     api_key: "bear"            # Placeholder for Bedrock-style configuration
#     max_tokens: 8192
#     temperature: 1.0
#   azure:
#     api_type: "azure"
#     model: "YOUR_MODEL_NAME"
#     base_url: "https://<your-endpoint>/openai/deployments/<deployment-id>"
#     api_key: "AZURE_API_KEY"
#     api_version: "2024-08-01-preview"
#     max_tokens: 8096
#     temperature: 0.0
#   ollama:
#     api_type: "ollama"
#     model: "llama3.2"
#     base_url: "http://localhost:11434/v1"
#     api_key: "ollama"
#     max_tokens: 4096
#     temperature: 0.0

# ================== Browser settings (browser) ==================
# browser:
#   headless: false              # Run headless?
#   disable_security: true       # Disable security policies (default: true)
#   extra_chromium_args: []      # Extra Chromium startup arguments
#   chrome_instance_path: ""     # Path to local Chrome executable
#   wss_url: ""                  # Connect to an existing browser via WebSocket
#   cdp_url: ""                  # Connect via CDP
#   proxy:                       # Proxy (optional)
#     server: "http://proxy:port"
#     username: "gpt-4o"
#     password: "pass"

# ================== Search settings (search) ==================
search:
  engine: "google"                      # Preferred search engine
  fallback_engines: ["bing", "duckduckgo", "baidu"]  # Fallback order: Bing, then DuckDuckGo, then Baidu
  retry_delay: 60                        # Overall retry delay after all failures (seconds)
  max_retries: 3                         # Maximum number of retry rounds
  # Optional: default language and country; if unset, defaults to en/us
  # lang: "en"
  # country: "us"

# ================== Sandbox Settings (sandbox) ==================
sandbox:
  use_sandbox: false
  image: "python:3.11-slim"
  # work_dir: "/workspace"
  memory_limit: "1g"
  cpu_limit: 2.0
  timeout: 300
  network_enabled: true