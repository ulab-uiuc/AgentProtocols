"""
ProtoRouter Benchmark ä¸»è¿è¡Œè„šæœ¬
"""

import json
import os
import sys
import yaml
from typing import Dict, Any
import argparse

# Add src to path for importing utils
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from proto_router import ProtoRouterBenchmark
from evaluator import RouterBenchmarkEvaluator


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        # åˆ›å»ºé»˜è®¤é…ç½®
        default_config = {
            "model": {
                "type": "openai",  # or "local"
                "name": "gpt-4o",
                "temperature": 0.1,
                "openai_api_key": "your_api_key_here",
                "openai_base_url": "https://api.openai.com/v1"
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_path}")
        print("è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶å¹¶è®¾ç½®æ­£ç¡®çš„APIå¯†é’¥")
        sys.exit(1)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_dataset(data_path: str) -> list:
    """åŠ è½½æ•°æ®é›†"""
    with open(data_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def main():
    parser = argparse.ArgumentParser(description='è¿è¡ŒProtoRouter Benchmark')
    parser.add_argument('--config', default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data', default='data/data.json', help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output', default='results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†çš„åœºæ™¯æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output, exist_ok=True)
    
    # åŠ è½½é…ç½®å’Œæ•°æ®
    print("ğŸ”§ Loading configuration...")
    config = load_config(args.config)
    
    print("ğŸ“š Loading dataset...")
    dataset = load_dataset(args.data)
    
    if args.limit:
        dataset = dataset[:args.limit]
        print(f"âš ï¸  Limited to {args.limit} scenarios")
    
    print(f"ğŸ“Š Dataset contains {len(dataset)} scenarios")
    
    # Initialize system
    print("ğŸš€ Initializing ProtoRouter...")
    router = ProtoRouterBenchmark(config)
    evaluator = RouterBenchmarkEvaluator()
    
    # Process all scenarios
    print("\nğŸ”„ Processing scenarios...")
    evaluation_results = []
    
    for i, scenario in enumerate(dataset, 1):
        scenario_id = scenario.get("id", f"scenario_{i}")
        print(f"  [{i}/{len(dataset)}] Processing scenario: {scenario_id}")
        
        try:
            # LLM processes scenario
            llm_result = router.process_scenario(scenario)
            
            if llm_result["success"]:
                # Evaluate results
                ground_truth = scenario.get("ground_truth", {})
                eval_result = evaluator.evaluate_scenario(
                    scenario_id, 
                    llm_result["llm_response"], 
                    ground_truth
                )
                evaluation_results.append(eval_result)
                
                # Show detailed results for each module
                correct = eval_result["correct_modules"]
                total = eval_result["total_modules"]
                print(f"    ğŸ“Š Result: {correct}/{total} modules correct")
                
                # Show each module's selection vs ground truth
                for module_id, module_result in eval_result["module_results"].items():
                    llm_choice = module_result["llm_selection"]
                    gt_choice = module_result["ground_truth"]
                    is_correct = module_result["correct"]
                    status = "âœ…" if is_correct else "âŒ"
                    print(f"      Module {module_id}: {status} LLM={llm_choice}, GT={gt_choice}")
                    
                    # Show A2A/ACP confusion specifically
                    if module_result.get("a2a_acp_confusion", False):
                        print(f"        âš ï¸  A2A/ACP confusion detected")
            else:
                print(f"    âŒ Processing failed: {llm_result['error']}")
                
        except Exception as e:
            print(f"    ğŸ’¥ Exception: {str(e)}")
    
    # Calculate statistics
    print("\nğŸ“Š Calculating statistics...")
    statistics = evaluator.calculate_overall_statistics(evaluation_results)
    
    # Generate report
    print("\nğŸ“‹ Generating report...")
    report = evaluator.generate_report(statistics)
    
    # Save results
    results_file = os.path.join(args.output, "benchmark_results.json")
    report_file = os.path.join(args.output, "benchmark_report.txt")
    
    evaluator.save_results(statistics, results_file)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ Report saved to: {report_file}")
    
    # Show brief results
    print("\n" + "="*60)
    print("ğŸ¯ Brief Results:")
    overall = statistics["overall_statistics"]
    print(f"  Overall scenario accuracy: {overall['overall_scenario_accuracy']:.2%}")
    print(f"  Individual module accuracy: {overall['individual_module_accuracy']:.2%}")
    print(f"  A2A/ACP confusion count: {overall['a2a_acp_confusion_count']}")
    
    print("\nBy difficulty level:")
    for difficulty, stats in statistics["difficulty_statistics"].items():
        print(f"  {difficulty}: scenario {stats['scenario_accuracy']:.1%}, module {stats['module_accuracy']:.1%}")
    
    print("="*60)


if __name__ == "__main__":
    main()
