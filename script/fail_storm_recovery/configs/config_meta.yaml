scenario:
  name: "fail_storm_recovery_meta_llm_routing"
  protocol: "meta"
  agent_count: 8
  total_runtime: 1800.0  # 3 minutes total for testing
  fault_injection_time: 120.0  # First fault at 2 minutes
  recovery_duration: 60  # 1 minute recovery time
  kill_fraction: 0.375  # Kill 3 out of 8 agents each cycle
  heartbeat_interval: 10.0
  heartbeat_timeout: 30.0
  cyclic_faults: true  # Enable cyclic fault injection
  fault_cycle_interval: 120.0  # Every 2 minutes (120 seconds)
  agents_per_fault: 3  # Kill 3 agents per cycle
  normal_phase_duration: 120.0  # 2 minutes normal phase before first fault
  max_groups: 5000  # Increase max groups for longer test

llm:
  type: "local"
  model: "Qwen2.5-VL-72B-Instruct"
  openai_api_key: "dummy-key"
  openai_base_url: "http://127.0.0.1:8000/v1"
  temperature: 0.2
  max_tokens: 8192
  timeout: 30.0

# Meta Protocol specific - LLM routing configuration
meta:
  llm_routing_enabled: true
  performance_based_fallback: true
  protocol_selection_strategy: "llm_with_fallback"
  # Based on actual fail_storm_recovery performance data:
  # ANP: 61.0% success, 22.0% answer rate (Best accuracy)
  # Agora: 60.0% success, 20.0% answer rate (Best throughput)  
  # A2A: 59.6% success, 19.1% answer rate (High volume)
  # ACP: 59.0% success, 17.9% answer rate (Best recovery)

network:
  base_port: 9200
  heartbeat_interval: 5.0
  heartbeat_timeout: 12.0
  connection_timeout: 10.0

agents:
  base_port: 9200
  host: "127.0.0.1"

results:
  base_path: "results"
  save_intermediate: true
  save_final: true
  detailed_metrics: true

qa:
  enabled: true
  data_file: "docs/gaia_document.txt"
  max_groups: 1000
  batch_size: 50
  questions_per_agent: 25
  
shard_qa:
  coordinator_port: 8765
  coordinator_host: "127.0.0.1"
  
  # Shard configuration for QA processing
  shards:
    count: 4
    questions_per_shard: 250
    overlap_ratio: 0.1
    
  # Worker configuration  
  workers:
    per_shard: 2
    timeout: 30.0
    retry_attempts: 2