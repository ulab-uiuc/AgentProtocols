#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
S2æ¢é’ˆRunneré›†æˆæµ‹è¯•
éªŒè¯S2æ¢é’ˆåœ¨å®é™…runnerç¯å¢ƒä¸­çš„é›†æˆæ•ˆæœ
"""

import asyncio
import os
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
SAFETY_TECH = Path(__file__).parent
sys.path.insert(0, str(SAFETY_TECH.parent.parent))

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['A2A_USE_DIRECT_SEND'] = 'true'
os.environ['A2A_ENABLE_S2_PROBES'] = 'true'
os.environ['A2A_S2_PROBE_TYPE'] = 'comprehensive'

from script.safety_tech.core.backend_api import send_backend
from script.safety_tech.core.probe_config import create_comprehensive_probe_config


async def test_runner_integration():
    """æµ‹è¯•S2æ¢é’ˆåœ¨runnerç¯å¢ƒä¸­çš„é›†æˆ"""
    print("ğŸš€ å¼€å§‹S2æ¢é’ˆRunneré›†æˆæµ‹è¯•")
    print(f"ğŸ“¡ ç¯å¢ƒé…ç½®:")
    print(f"   A2A_USE_DIRECT_SEND: {os.environ.get('A2A_USE_DIRECT_SEND')}")
    print(f"   A2A_ENABLE_S2_PROBES: {os.environ.get('A2A_ENABLE_S2_PROBES')}")
    print(f"   A2A_S2_PROBE_TYPE: {os.environ.get('A2A_S2_PROBE_TYPE')}")
    
    # æ¨¡æ‹Ÿrunnerä¸­çš„åŒ»ç–—æ¡ˆä¾‹æ•°æ®
    medical_cases = [
        {
            'case_id': 'CASE_001',
            'patient_info': 'æ‚£è€…: å¼ ä¸‰, å¹´é¾„: 45å², ä¸»è¯‰: èƒ¸ç—›',
            'initial_question': 'æ‚£è€…å‡ºç°æŒç»­æ€§èƒ¸ç—›ï¼Œä¼´æœ‰å‘¼å¸å›°éš¾ï¼Œè¡€å‹160/90ï¼Œå¿ƒç”µå›¾æ˜¾ç¤ºSTæ®µæŠ¬é«˜ï¼Œè¯·ååŠ©è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚'
        }
    ]
    
    # æµ‹è¯•é…ç½®
    test_configs = {
        'TLSé™çº§æ¢é’ˆ': {'A2A_S2_PROBE_TYPE': 'tls_downgrade'},
        'é‡æ”¾æ”»å‡»æ¢é’ˆ': {'A2A_S2_PROBE_TYPE': 'replay_attack'}, 
        'DIDä¼ªé€ æ¢é’ˆ': {'A2A_S2_PROBE_TYPE': 'did_spoofing'},
        'ç»¼åˆæ¢é’ˆ': {'A2A_S2_PROBE_TYPE': 'comprehensive'}
    }
    
    results = {}
    
    for test_name, config in test_configs.items():
        print(f"\nğŸ” === æµ‹è¯• {test_name} ===")
        
        # æ›´æ–°ç¯å¢ƒå˜é‡
        for key, value in config.items():
            os.environ[key] = value
            
        # æ¨¡æ‹Ÿrunnerä¸­çš„å¯¹è¯å¾ªç¯
        total_attempted = 0
        total_success = 0
        total_latencies = []
        probe_results_collection = []
        
        for case in medical_cases:
            print(f"\nğŸ“‹ å¤„ç†æ¡ˆä¾‹: {case['case_id']}")
            
            # æ¨¡æ‹Ÿ5è½®å¯¹è¯
            for r in range(5):
                total_attempted += 1
                text = f"[Round {r+1}] {case['initial_question'][:100]}..."
                
                try:
                    start_time = time.time()
                    _mid = f"msg_{int(time.time()*1000)}"
                    _cid = f"corr_{int(time.time()*1000)}_{r}"
                    
                    # ä½¿ç”¨ç»¼åˆæ¢é’ˆé…ç½®
                    probe_config = create_comprehensive_probe_config().to_dict()
                    
                    # æ¨¡æ‹Ÿæ•°æ®é¢ç›´è¿å‘é€
                    payload = {
                        'sender_id': 'A2A_Doctor_A',
                        'receiver_id': 'A2A_Doctor_B', 
                        'text': text,
                        'message_id': _mid
                    }
                    
                    result = await send_backend(
                        'a2a', 
                        'http://127.0.0.1:8001',  # è™šæ‹Ÿç«¯ç‚¹
                        payload, 
                        _cid, 
                        probe_config=probe_config
                    )
                    
                    latency_ms = (time.time() - start_time) * 1000
                    total_latencies.append(latency_ms)
                    
                    # æ£€æŸ¥æ¢é’ˆç»“æœ
                    probe_results = result.get('probe_results', {})
                    if probe_results:
                        probe_results_collection.append(probe_results)
                        total_success += 1
                        print(f"   âœ… Round {r+1}: æˆåŠŸ ({latency_ms:.1f}ms)")
                        print(f"      æ¢é’ˆç»“æœ: {probe_results}")
                    else:
                        print(f"   âŒ Round {r+1}: æ— æ¢é’ˆç»“æœ")
                        
                except Exception as e:
                    print(f"   âŒ Round {r+1}: é”™è¯¯ - {str(e)}")
        
        # ç»Ÿè®¡ç»“æœ
        success_rate = total_success / total_attempted if total_attempted > 0 else 0
        avg_latency = sum(total_latencies) / len(total_latencies) if total_latencies else 0
        
        # èšåˆæ¢é’ˆç»“æœï¼ˆæ¨¡æ‹Ÿrunnerä¸­çš„ç»Ÿè®¡é€»è¾‘ï¼‰
        aggregated_probes = {}
        if probe_results_collection:
            aggregated_probes = {
                'total_probes': len(probe_results_collection),
                'tls_downgrade_attempts': len([p for p in probe_results_collection if p.get('tls_downgrade')]),
                'replay_attempts': len([p for p in probe_results_collection if p.get('replay_attack')]),
                'did_spoofing_attempts': len([p for p in probe_results_collection if p.get('did_spoofing')]),
                'plaintext_bytes_detected': sum(p.get('plaintext_detected', 0) for p in probe_results_collection),
                'sensitive_keywords_total': sum(len(p.get('sensitive_keywords_detected', [])) for p in probe_results_collection),
                'avg_sensitive_score': sum(p.get('sensitive_data_score', 0) for p in probe_results_collection) / len(probe_results_collection)
            }
        
        results[test_name] = {
            'success_rate': success_rate,
            'avg_latency_ms': avg_latency,
            'probe_results': aggregated_probes
        }
        
        print(f"\nğŸ“Š {test_name} ç»“æœ:")
        print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency:.1f}ms")
        print(f"   æ¢é’ˆç»Ÿè®¡: {aggregated_probes}")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print(f"\nğŸ“‹ === S2æ¢é’ˆRunneré›†æˆæµ‹è¯•æŠ¥å‘Š ===")
    
    for test_name, result in results.items():
        print(f"\nğŸ”¸ {test_name}:")
        print(f"  æˆåŠŸç‡: {result['success_rate']:.1%}")
        print(f"  å¹³å‡å»¶è¿Ÿ: {result['avg_latency_ms']:.1f}ms")
        
        probe_stats = result['probe_results']
        if probe_stats:
            print(f"  æ¢é’ˆç»Ÿè®¡:")
            print(f"    æ€»æ¢é’ˆæ¬¡æ•°: {probe_stats.get('total_probes', 0)}")
            print(f"    TLSé™çº§å°è¯•: {probe_stats.get('tls_downgrade_attempts', 0)}")
            print(f"    é‡æ”¾æ”»å‡»å°è¯•: {probe_stats.get('replay_attempts', 0)}")
            print(f"    DIDä¼ªé€ å°è¯•: {probe_stats.get('did_spoofing_attempts', 0)}")
            print(f"    æ˜æ–‡å­—èŠ‚æ£€æµ‹: {probe_stats.get('plaintext_bytes_detected', 0)}")
            print(f"    æ•æ„Ÿå…³é”®å­—æ€»æ•°: {probe_stats.get('sensitive_keywords_total', 0)}")
            print(f"    å¹³å‡æ•æ„Ÿåº¦è¯„åˆ†: {probe_stats.get('avg_sensitive_score', 0):.1f}")
    
    # è¯„ä¼°é›†æˆæ•ˆæœ
    total_success_rate = sum(r['success_rate'] for r in results.values()) / len(results)
    avg_latency_impact = sum(r['avg_latency_ms'] for r in results.values()) / len(results)
    
    print(f"\nğŸ¯ === é›†æˆæ•ˆæœè¯„ä¼° ===")
    print(f"æ€»ä½“æˆåŠŸç‡: {total_success_rate:.1%}")
    print(f"å¹³å‡å»¶è¿Ÿå½±å“: {avg_latency_impact:.1f}ms")
    
    if total_success_rate >= 0.9:
        print("ğŸ‰ S2æ¢é’ˆRunneré›†æˆæ•ˆæœä¼˜ç§€ï¼")
    elif total_success_rate >= 0.7:
        print("âœ… S2æ¢é’ˆRunneré›†æˆæ•ˆæœè‰¯å¥½")
    else:
        print("âš ï¸ S2æ¢é’ˆRunneré›†æˆéœ€è¦ä¼˜åŒ–")
        
    if avg_latency_impact < 200:
        print("âš¡ å»¶è¿Ÿå½±å“åœ¨å¯æ¥å—èŒƒå›´å†…")
    else:
        print("ğŸŒ å»¶è¿Ÿå½±å“è¾ƒå¤§ï¼Œéœ€è¦ä¼˜åŒ–")
    
    return results


if __name__ == "__main__":
    asyncio.run(test_runner_integration())
